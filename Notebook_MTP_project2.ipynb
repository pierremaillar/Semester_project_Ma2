{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d965aeb1",
   "metadata": {},
   "source": [
    "# Project 2 : Classification of Hsp70 proteins into taxonomic groups based on their amino acid sequences\n",
    " This notebook shows how we prepare the dataset, train the model and analyze the features importance. This notebook does not contain the code to compute the variation of the results (performances and feature importance), as this simply repeats what this notebook does but 30 times to obtain a statistical representation of the computed quantities. The code to be able to compute the mean and standard deviation (given in the report), is still provided in an additional notebook called `Notebook_MTP_project2_results.ipynb` . Some cells may take a lot of time to run (especially the hyperparameters optimizations) so please be aware of the estimated time indications. These time indications have been estimated using the CPU of a standard laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c193d",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Import libraries](#1.-Import-libraries)\n",
    "2. [Importing and treating the dataset](#2.-Importing-and-treating-the-data-set)\n",
    "3. [T-distributed stochatic neighbor embedding](#3.-TSNE)\n",
    "4. [Mutual information](#4.-Mutual-information)\n",
    "5. [Training of the random forest](#5.-Random-Forest)\n",
    "6. [Training of the Neural Network](#6.-Neural-Network)\n",
    "7. [Features importance](#7.-Features-importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f690",
   "metadata": {},
   "source": [
    "## 1. Import libraries and install requirements\n",
    "The first cell download all the required libraries. See README.md for more informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17da2d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pierr\\AppData\\Roaming\\Python\\Python311\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "laptrack 0.16.0 requires pandas<2.0.0,>=1.3.1, but you have pandas 2.2.0 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.1.1 requires torch==2.1.1, but you have torch 2.2.0 which is incompatible.\n",
      "torchvision 0.16.1 requires torch==2.1.1, but you have torch 2.2.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc847907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from create_batch import *\n",
    "from datatreatment import *\n",
    "from neuralnet import *\n",
    "from visualisations import *\n",
    "from mutual_info import *\n",
    "from TSNE import *\n",
    "from train_rdf import *\n",
    "from rdf_hyperpara_opti import *\n",
    "from nn_hyperpara_opti import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c8ad7",
   "metadata": {},
   "source": [
    "## 2. Importing and treating the dataset\n",
    "This section performs the feature embedding and selects the level of taxonomy where the model will be trained. Make sure to change the path_to_dataset variable. This section can be run in more or less 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7cd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset =\"C:\\\\Users\\\\pierr\\\\OneDrive\\\\Bureau\\\\Projet_ML\\\\dataset_hsp70_tax.csv\"\n",
    "hsp70 = importing_data(path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c4db51d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m level3, level3_categ\u001b[38;5;241m=\u001b[39m get_data(hsp70, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpisthokonta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m level3\u001b[38;5;241m=\u001b[39mencode01(level3)\n\u001b[0;32m      3\u001b[0m level3\u001b[38;5;241m=\u001b[39mcategory_to_int(level3,level3_categ)\n",
      "File \u001b[1;32m~\\OneDrive\\Bureau\\Projet_ML\\ml-project-2-mtp\\datatreatment.py:64\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(data, level, number_category, type_classification)\u001b[0m\n\u001b[0;32m     61\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_niv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Extract the specified taxonomic level\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxonomic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_word_at_position(x, level \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_classification \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotspecified\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevious level\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxonomic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_word_at_position(x, level \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4898\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4899\u001b[0m         func,\n\u001b[0;32m   4900\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4901\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4902\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4903\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4904\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\OneDrive\\Bureau\\Projet_ML\\ml-project-2-mtp\\datatreatment.py:64\u001b[0m, in \u001b[0;36mget_data.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     61\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_niv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Extract the specified taxonomic level\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxonomic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_word_at_position(x, level \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_classification \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotspecified\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevious level\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxonomic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_word_at_position(x, level \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\OneDrive\\Bureau\\Projet_ML\\ml-project-2-mtp\\datatreatment.py:14\u001b[0m, in \u001b[0;36mextract_word_at_position\u001b[1;34m(X, p)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_word_at_position\u001b[39m(X, p):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract a word at a specific position in a comma-separated string.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m        str: Word at the specified position.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)[p]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "level3, level3_categ= get_data(hsp70, 4, 4,\"Opisthokonta\")\n",
    "level3=encode01(level3)\n",
    "level3=category_to_int(level3,level3_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5110dc",
   "metadata": {},
   "source": [
    "## 3. TSNE\n",
    "This section performs the T-distributed stochatic neighbor embedding analysis. If done on the entire dataset, it takes 30 minutes. It can also be done on a smaller set by changing the argument of the split_dataset() function.\n",
    "`X_tsne,labels_tsne,_,_,_,_=split_dataset(level3, 0.1, 0, 0)` will only execute the analysis on 10% of the dataset. Note that TSNE contains randomness, so results may vary slightly from the report and if you change the size of the subset you may need to adapt the complexity (typically a complexitiy of 150 works great with 50% of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6384d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 350\n",
    "X_tsne, labels_tsne, _, _, _, _=split_dataset(level3, 1, 0, 0)\n",
    "tsne = compute_tsne(X_tsne, perplexity)\n",
    "plot_tsne(tsne, labels_tsne,level3_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e96e5a",
   "metadata": {},
   "source": [
    "## 4. Mutual information\n",
    "This section computes the mutual information of the features in the dataset. The computation takes approximately 2 hours but can be done on a subset of the entire dataset. `mutual_data = compute_mutual_info(level3,0.1)` will perform the computation on 10 % of the dataset. We also provide a txt file `mutual_data.txt`that contains the calculated values. So, if you want to proceed without waiting for the computation (`mutual_data` is used further), just run the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_data = compute_mutual_info(level3,1)\n",
    "np.savetxt('mutual_data.txt', mutual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_data = np.loadtxt('mutual_data.txt',dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91be445",
   "metadata": {},
   "source": [
    "## 5. Random Forest\n",
    "This section does the training and hyperparameters optimization of the Random Forest. The hyperparameters optimization for the grid done below takes 14 hours to run. The second cell contains the optimal parameters discribed in the report, so skip the first cell if you don't want to go through the hyperparameter optimizations. The training of the Random Forest, with the hyperparameters that we selected, takes 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "        'n_estimators': [10,50,100],\n",
    "        'max_depth':[200,300,400,500,600],\n",
    "        'max_features':[100,200,400,600],\n",
    "        'bootstrap':[False,True],\n",
    "        'class_weight':['balanced'], \n",
    "        'min_samples_leaf':[5,10,20]\n",
    "        }\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.8, 0.1, 0.1)\n",
    "\n",
    "\n",
    "best_para = optimize_hyperparameters_rf(train, train_label, test, test_label,parameter_grid,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310520b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "val_pred = model_rdf.predict(val)\n",
    "convolution_matrix(val_label,val_pred,level3_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1fa3b",
   "metadata": {},
   "source": [
    "## 6. Neural Network\n",
    "This section does the training and hyperparameters optimization of the Neural Network. The hyperparameters optimization for the grid done below takes 14 hours to run using a GPU. The second cell contains the optimal parameters discribed in the report, so skip the first cell if you don't want to go through the hyperparameter optimization. Also, don't execute the second one if you want to use your optimized hyperparameters. The training of the Neural Network, with the hyperparameters that we selected, takes 20 seconds if a GPU is used and 4 minutes the computation is done on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'layer_dim': [128,256],\n",
    "        'number_hidden_layer': [4,5],\n",
    "        'dropout_prob': [0.2,0.25,0.3],\n",
    "        'l2_regu': [0.00001,0.0001],\n",
    "        'weight_decay': [0.001,0.0001],\n",
    "        'learning_rate':[0.001, 0.0001],\n",
    "        'batch_size':[256, 128, 64],\n",
    "        'num_epochs':[5 ,10 ,15]\n",
    "        }\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.8, 0.1, 0.1)\n",
    "cv=5\n",
    "best_params_nn, mean_f1_score_nn, std_f1_score_nn = optimize_hyperparameters_nn(train, train_label, val, val_label, cv,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.8, 0.1, 0.1)\n",
    "train = torch.cat([train,val],dim = 0)\n",
    "train_label = torch.cat([train_label,val_label],dim = 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1353d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    \n",
    "test_outputs = model_neural(test)\n",
    "test_pred = test_outputs.argmax(dim=1)\n",
    "convolution_matrix(test_label,test_pred,level3_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21688a",
   "metadata": {},
   "source": [
    "## 7. Features importance\n",
    "This section computes the features importance in both model and standardizes them. They are then plotted in comparison with the mutual informations to be able to see which region of the sequence is more important for the task. Note that the signal is filtered through a moving average of size `smoothness`. The first cell contains the feature importance calculated by all models and the second is just the feature importance in the neural network However, the importance is divided between each class to observe whether they match. We provide 2 .pkl files to be able to visualize the importances without the need to run the entire notebook. If you want to compute the features importance of the models trained, do not run the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_impo = pd.read_pickle('features_impo.pkl')\n",
    "features_impo_nn = pd.read_pickle('features_impo_nn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ed773",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_impo = feature_importances(model_rdf, model_neural, mutual_data, level3, smoothness = 30)\n",
    "features_impo.to_pickle(\"features_impo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62256e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_impo_nn = features_importances_nn(model_neural, level3, smoothness = 30, plot=1)\n",
    "features_impo_nn.to_pickle(\"features_impo_nn.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
