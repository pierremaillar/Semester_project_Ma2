{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fa0ee6",
   "metadata": {},
   "source": [
    "# Project 2 Classification of Hsp70 proteins into taxonomic groups based on their amino acid sequences\n",
    " This notebook is only to compute the performance of each model provide in the report. Please refere the other notebook `Notebook_MTP_project2` for a description of the code and complete access to the code of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2cbc9",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc847907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "\n",
    "from create_batch import *\n",
    "from datatreatment import *\n",
    "from neuralnet import *\n",
    "from visualisations import *\n",
    "from mutual_info import *\n",
    "from TSNE import *\n",
    "from train_rdf import *\n",
    "from rdf_hyperpara_opti import *\n",
    "from nn_hyperpara_opti import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c8ad7",
   "metadata": {},
   "source": [
    "## Importing and treating the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee7cd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_dataset =\"C:\\\\Users\\\\pierr\\\\OneDrive\\\\Bureau\\\\Semester_project_MA2\\\\dataset_hsp70_tax.csv\" \n",
    "path_to_dataset =\"C:\\\\Users\\\\pierr\\\\Desktop\\\\Semester_project_Ma2\\\\dataset_hsp70_tax\\\\dataset_hsp70_tax.csv\"\n",
    "hsp70 = importing_data(path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c4db51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Opisthokonta : \n",
      "{25989}\n",
      "---------\n",
      "number of Pseudomonadota : \n",
      "{23889}\n",
      "---------\n",
      "number of Others : \n",
      "{48628}\n",
      "---------\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 450. MiB for an array with shape (98506, 599) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m level3, level3_categ\u001b[38;5;241m=\u001b[39m get_data(hsp70, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m,Use_Others\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m level3\u001b[38;5;241m=\u001b[39mencode01(level3)\n\u001b[0;32m      3\u001b[0m level3\u001b[38;5;241m=\u001b[39mcategory_to_int(level3,level3_categ)\n",
      "File \u001b[1;32m~\\Desktop\\Semester_project_Ma2\\datatreatment.py:114\u001b[0m, in \u001b[0;36mencode01\u001b[1;34m(data_to_encode)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode a column of sequences into binary columns (0/1 encoding).\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    pd.DataFrame: Encoded data with binary columns.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m#split the sequence into different columns\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m split_sequence \u001b[38;5;241m=\u001b[39m data_to_encode[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHsp70_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mlist\u001b[39m(x)))\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Rename the columns to pos_1, pos_2, ...\u001b[39;00m\n\u001b[0;32m    117\u001b[0m split_sequence\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(split_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4909\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4910\u001b[0m         func,\n\u001b[0;32m   4911\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4912\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4913\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4914\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4915\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1514\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m-> 1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor(mapped, index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   1517\u001b[0m         obj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1518\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:840\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    839\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 840\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    843\u001b[0m         data,\n\u001b[0;32m    844\u001b[0m         columns,\n\u001b[0;32m    845\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    846\u001b[0m         dtype,\n\u001b[0;32m    847\u001b[0m     )\n\u001b[0;32m    848\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    849\u001b[0m         arrays,\n\u001b[0;32m    850\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    853\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    854\u001b[0m     )\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:839\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m--> 839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_series_to_arrays(data, columns)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;66;03m# last ditch effort\u001b[39;00m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:887\u001b[0m, in \u001b[0;36m_list_of_series_to_arrays\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m    884\u001b[0m     values \u001b[38;5;241m=\u001b[39m extract_array(s, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    885\u001b[0m     aligned_values\u001b[38;5;241m.\u001b[39mappend(algorithms\u001b[38;5;241m.\u001b[39mtake_nd(values, indexer))\n\u001b[1;32m--> 887\u001b[0m content \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(aligned_values)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 450. MiB for an array with shape (98506, 599) and data type object"
     ]
    }
   ],
   "source": [
    "level3, level3_categ= get_data(hsp70, 3, 3,Use_Others=True)\n",
    "level3=encode01(level3)\n",
    "level3=category_to_int(level3,level3_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f4c9ff-e0fb-473b-afda-1c9fd1c26d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level 3</th>\n",
       "      <th>pos_1_-</th>\n",
       "      <th>pos_1_A</th>\n",
       "      <th>pos_1_I</th>\n",
       "      <th>pos_1_M</th>\n",
       "      <th>pos_1_V</th>\n",
       "      <th>pos_2_-</th>\n",
       "      <th>pos_2_A</th>\n",
       "      <th>pos_2_C</th>\n",
       "      <th>pos_2_F</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_598_R</th>\n",
       "      <th>pos_598_S</th>\n",
       "      <th>pos_598_T</th>\n",
       "      <th>pos_598_V</th>\n",
       "      <th>pos_598_X</th>\n",
       "      <th>pos_598_Y</th>\n",
       "      <th>pos_599_-</th>\n",
       "      <th>pos_599_E</th>\n",
       "      <th>pos_599_K</th>\n",
       "      <th>pos_599_Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98501</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98502</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98503</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98504</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98505</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98506 rows Ã— 12927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      level 3  pos_1_-  pos_1_A  pos_1_I  pos_1_M  pos_1_V  pos_2_-  pos_2_A  \\\n",
       "0           0    False    False    False    False     True    False    False   \n",
       "1           0    False     True    False    False    False    False    False   \n",
       "2           0    False     True    False    False    False    False    False   \n",
       "3           0    False    False    False    False     True    False    False   \n",
       "4           0    False     True    False    False    False    False    False   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "98501       0    False    False    False    False     True    False    False   \n",
       "98502       0    False     True    False    False    False    False    False   \n",
       "98503       0    False    False    False    False     True    False    False   \n",
       "98504       0    False     True    False    False    False    False    False   \n",
       "98505       1     True    False    False    False    False    False    False   \n",
       "\n",
       "       pos_2_C  pos_2_F  ...  pos_598_R  pos_598_S  pos_598_T  pos_598_V  \\\n",
       "0        False    False  ...      False      False      False      False   \n",
       "1        False    False  ...      False      False      False      False   \n",
       "2        False    False  ...      False      False      False      False   \n",
       "3        False    False  ...      False      False      False      False   \n",
       "4        False    False  ...      False      False      False      False   \n",
       "...        ...      ...  ...        ...        ...        ...        ...   \n",
       "98501    False    False  ...      False      False      False      False   \n",
       "98502    False    False  ...      False      False      False      False   \n",
       "98503    False    False  ...       True      False      False      False   \n",
       "98504    False    False  ...      False      False      False      False   \n",
       "98505    False    False  ...      False      False      False      False   \n",
       "\n",
       "       pos_598_X  pos_598_Y  pos_599_-  pos_599_E  pos_599_K  pos_599_Q  \n",
       "0          False       True       True      False      False      False  \n",
       "1          False       True      False      False      False       True  \n",
       "2          False       True      False      False      False       True  \n",
       "3          False       True       True      False      False      False  \n",
       "4          False       True      False      False      False       True  \n",
       "...          ...        ...        ...        ...        ...        ...  \n",
       "98501      False      False       True      False      False      False  \n",
       "98502      False       True       True      False      False      False  \n",
       "98503      False      False       True      False      False      False  \n",
       "98504      False      False       True      False      False      False  \n",
       "98505      False      False       True      False      False      False  \n",
       "\n",
       "[98506 rows x 12927 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036dbff",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85652bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_data = np.loadtxt('mutual_data.txt',dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91be445",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8d7b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters are: {'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}\n",
      "Training done in 146.54566621780396\n",
      "Parameters are: {'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}\n",
      "Training done in 147.403826713562\n",
      "[0.9919688479825668, 0.9908731604541506]\n",
      "0.000547843764208078\n",
      "0.9914210042183587\n"
     ]
    }
   ],
   "source": [
    "#best score\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9,0.1,0)\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    test_pred = model_rdf.predict(test)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "\n",
    "print(f\"F1 score of the random forest: {np.mean(F1)} with std: {np.std(F1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with bootstrap\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': True, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9,0.1,0)\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    test_pred = model_rdf.predict(test)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "\n",
    "print(f\"F1 score of the random forest: {np.mean(F1)} with std: {np.std(F1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without 'balanced' option\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'min_samples_leaf': 5}, 0, 0)\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9,0.1,0)\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    test_pred = model_rdf.predict(test)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "\n",
    "print(f\"F1 score of the random forest: {np.mean(F1)} with std: {np.std(F1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1fa3b",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7105c86b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     18\u001b[0m     model_neural \u001b[38;5;241m=\u001b[39m ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n\u001b[1;32m---> 19\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_neural\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m learning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\u001b[38;5;66;03m#lr : learning rate\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n\u001b[0;32m     21\u001b[0m     test_outputs \u001b[38;5;241m=\u001b[39m model_neural(test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._dynamo'"
     ]
    }
   ],
   "source": [
    "# best score\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "786e9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.0628\n",
      "Epoch [1/10], Validation Loss: 0.0733\n",
      "Epoch [1/10], Validation Accuracy: 97.5529%\n",
      "Epoch [2/10], Training Loss: 0.1155\n",
      "Epoch [2/10], Validation Loss: 0.0493\n",
      "Epoch [2/10], Validation Accuracy: 98.8177%\n",
      "Epoch [3/10], Training Loss: 0.0598\n",
      "Epoch [3/10], Validation Loss: 0.0405\n",
      "Epoch [3/10], Validation Accuracy: 98.8727%\n",
      "Epoch [4/10], Training Loss: 0.0553\n",
      "Epoch [4/10], Validation Loss: 0.0350\n",
      "Epoch [4/10], Validation Accuracy: 99.1202%\n",
      "Epoch [5/10], Training Loss: 0.0205\n",
      "Epoch [5/10], Validation Loss: 0.0374\n",
      "Epoch [5/10], Validation Accuracy: 99.1202%\n",
      "Epoch [6/10], Training Loss: 0.0135\n",
      "Epoch [6/10], Validation Loss: 0.0381\n",
      "Epoch [6/10], Validation Accuracy: 98.9827%\n",
      "Epoch [7/10], Training Loss: 0.0589\n",
      "Epoch [7/10], Validation Loss: 0.0364\n",
      "Epoch [7/10], Validation Accuracy: 99.1751%\n",
      "Epoch [8/10], Training Loss: 0.0769\n",
      "Epoch [8/10], Validation Loss: 0.0395\n",
      "Epoch [8/10], Validation Accuracy: 99.0652%\n",
      "Epoch [9/10], Training Loss: 0.0244\n",
      "Epoch [9/10], Validation Loss: 0.0339\n",
      "Epoch [9/10], Validation Accuracy: 99.1202%\n",
      "Epoch [10/10], Training Loss: 0.0168\n",
      "Epoch [10/10], Validation Loss: 0.0385\n",
      "Epoch [10/10], Validation Accuracy: 99.1476%\n",
      "Epoch [1/10], Training Loss: 0.1036\n",
      "Epoch [1/10], Validation Loss: 0.0564\n",
      "Epoch [1/10], Validation Accuracy: 98.4603%\n",
      "Epoch [2/10], Training Loss: 0.0200\n",
      "Epoch [2/10], Validation Loss: 0.0431\n",
      "Epoch [2/10], Validation Accuracy: 98.7077%\n",
      "Epoch [3/10], Training Loss: 0.0177\n",
      "Epoch [3/10], Validation Loss: 0.0437\n",
      "Epoch [3/10], Validation Accuracy: 98.9002%\n",
      "Epoch [4/10], Training Loss: 0.0310\n",
      "Epoch [4/10], Validation Loss: 0.0339\n",
      "Epoch [4/10], Validation Accuracy: 99.1751%\n",
      "Epoch [5/10], Training Loss: 0.0129\n",
      "Epoch [5/10], Validation Loss: 0.0366\n",
      "Epoch [5/10], Validation Accuracy: 99.0102%\n",
      "Epoch [6/10], Training Loss: 0.0319\n",
      "Epoch [6/10], Validation Loss: 0.0460\n",
      "Epoch [6/10], Validation Accuracy: 98.7627%\n",
      "Epoch [7/10], Training Loss: 0.0551\n",
      "Epoch [7/10], Validation Loss: 0.0488\n",
      "Epoch [7/10], Validation Accuracy: 98.4878%\n",
      "Epoch [8/10], Training Loss: 0.0814\n",
      "Epoch [8/10], Validation Loss: 0.0519\n",
      "Epoch [8/10], Validation Accuracy: 98.7077%\n",
      "Epoch [9/10], Training Loss: 0.0127\n",
      "Epoch [9/10], Validation Loss: 0.0343\n",
      "Epoch [9/10], Validation Accuracy: 99.1202%\n",
      "Epoch [10/10], Training Loss: 0.0165\n",
      "Epoch [10/10], Validation Loss: 0.0349\n",
      "Epoch [10/10], Validation Accuracy: 99.2026%\n",
      "[0.9913803041880255, 0.992075362645065]\n",
      "0.00034752922851977175\n",
      "0.9917278334165452\n"
     ]
    }
   ],
   "source": [
    "#without dropout\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without l2-rugularization\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 0, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without weight_decay\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff091f47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     19\u001b[0m     model_neural \u001b[38;5;241m=\u001b[39m ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n\u001b[1;32m---> 20\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_neural\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m learning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\u001b[38;5;66;03m#lr : learning rate\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n\u001b[0;32m     22\u001b[0m     test_outputs \u001b[38;5;241m=\u001b[39m model_neural(test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._dynamo'"
     ]
    }
   ],
   "source": [
    "#without batches\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "batch_size = train.shape[0]\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21688a",
   "metadata": {},
   "source": [
    "### Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54521ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_nn = {'layer_dim': 128, 'number_hidden_layer': 3, 'dropout_prob': 0.2, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.0001, 'batch_size': 256, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)\n",
    "\n",
    "dfs = []\n",
    "for i in range(30):\n",
    "    \n",
    "    mutual_data = compute_mutual_info(level3,0.5)\n",
    "    \n",
    "    train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "    \n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    \n",
    "    dfs.append(feature_importances(model_rdf, model_neural, mutual_data, level3, smoothness = 40, plot = 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "arrays = [df.to_numpy() for df in dfs]\n",
    "\n",
    "stacked_array = np.stack(arrays, axis=0)\n",
    "mean_values = np.mean(stacked_array, axis=0)\n",
    "std_values = np.std(stacked_array, axis=0)\n",
    "\n",
    "\n",
    "np.savetxt('mean.txt', mean_values)\n",
    "np.savetxt('std.txt', std_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(mean_values.shape[0]), mean_values[:, 0],label='Neural Network', color=\"red\", lw=2) \n",
    "\n",
    "plt.fill_between(\n",
    "    np.arange(mean_values.shape[0]),\n",
    "    mean_values[:, 0] - std_values[:, 0],\n",
    "    mean_values[:, 0] + std_values[:, 0],\n",
    "    color=\"red\",\n",
    "    alpha=0.3 \n",
    ")\n",
    "plt.plot(np.arange(mean_values.shape[0]), mean_values[:, 1], label='Mutual Information', color=\"green\", lw=2) \n",
    "plt.fill_between(\n",
    "    np.arange(mean_values.shape[0]),\n",
    "    mean_values[:, 1] - std_values[:, 1],\n",
    "    mean_values[:, 1] + std_values[:, 1],\n",
    "    color=\"green\",\n",
    "    alpha=0.3 \n",
    ")\n",
    "plt.plot(np.arange(mean_values.shape[0]), mean_values[:, 2], label='Random forest', color=\"blue\", lw=2) \n",
    "plt.fill_between(\n",
    "    np.arange(mean_values.shape[0]),\n",
    "    mean_values[:, 2] - std_values[:, 2],\n",
    "    mean_values[:, 2] + std_values[:, 2],\n",
    "    color=\"blue\",\n",
    "    alpha=0.3 \n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel('Positions')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Position importances')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
