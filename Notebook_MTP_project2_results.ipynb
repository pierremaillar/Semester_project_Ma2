{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fa0ee6",
   "metadata": {},
   "source": [
    "# Project 2 Classification of Hsp70 proteins into taxonomic groups based on their amino acid sequences\n",
    " This notebook is only to compute the performance of each model provide in the report. Please refere the other notebook `Notebook_MTP_project2` for a description of the code and complete access to the code of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2cbc9",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc847907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "\n",
    "from create_batch import *\n",
    "from datatreatment import *\n",
    "from neuralnet import *\n",
    "from visualisations import *\n",
    "from mutual_info import *\n",
    "from TSNE import *\n",
    "from train_rdf import *\n",
    "from rdf_hyperpara_opti import *\n",
    "from nn_hyperpara_opti import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c8ad7",
   "metadata": {},
   "source": [
    "## Importing and treating the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7cd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset =\"C:\\\\Users\\\\pierr\\\\OneDrive\\\\Bureau\\\\Projet_ML\\\\dataset_hsp70_tax.csv\"\n",
    "hsp70 = importing_data(path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4db51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Opisthokonta : \n",
      "{25989}\n",
      "---------\n",
      "number of Viridiplantae : \n",
      "{6861}\n",
      "---------\n",
      "number of Sar : \n",
      "{2241}\n",
      "---------\n",
      "number of Others : \n",
      "{1286}\n",
      "---------\n",
      "shape of dataframe : (36377, 12834)\n"
     ]
    }
   ],
   "source": [
    "level3, level3_categ= get_data(hsp70, 3, 4,\"Eukaryota\")\n",
    "level3=encode01(level3)\n",
    "level3=category_to_int(level3,level3_categ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036dbff",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85652bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_data = np.loadtxt('mutual_data.txt',dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91be445",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8d7b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters are: {'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}\n",
      "Training done in 146.54566621780396\n",
      "Parameters are: {'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}\n",
      "Training done in 147.403826713562\n",
      "[0.9919688479825668, 0.9908731604541506]\n",
      "0.000547843764208078\n",
      "0.9914210042183587\n"
     ]
    }
   ],
   "source": [
    "#best score\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9,0.1,0)\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    test_pred = model_rdf.predict(test)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "\n",
    "print(f\"F1 score of the random forest: {np.mean(F1)} with std: {np.std(F1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with bootstrap\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': True, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9,0.1,0)\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    test_pred = model_rdf.predict(test)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "\n",
    "print(f\"F1 score of the random forest: {np.mean(F1)} with std: {np.std(F1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without 'balanced' option\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'min_samples_leaf': 5}, 0, 0)\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9,0.1,0)\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    test_pred = model_rdf.predict(test)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "\n",
    "print(f\"F1 score of the random forest: {np.mean(F1)} with std: {np.std(F1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1fa3b",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7105c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best score\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "786e9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.0628\n",
      "Epoch [1/10], Validation Loss: 0.0733\n",
      "Epoch [1/10], Validation Accuracy: 97.5529%\n",
      "Epoch [2/10], Training Loss: 0.1155\n",
      "Epoch [2/10], Validation Loss: 0.0493\n",
      "Epoch [2/10], Validation Accuracy: 98.8177%\n",
      "Epoch [3/10], Training Loss: 0.0598\n",
      "Epoch [3/10], Validation Loss: 0.0405\n",
      "Epoch [3/10], Validation Accuracy: 98.8727%\n",
      "Epoch [4/10], Training Loss: 0.0553\n",
      "Epoch [4/10], Validation Loss: 0.0350\n",
      "Epoch [4/10], Validation Accuracy: 99.1202%\n",
      "Epoch [5/10], Training Loss: 0.0205\n",
      "Epoch [5/10], Validation Loss: 0.0374\n",
      "Epoch [5/10], Validation Accuracy: 99.1202%\n",
      "Epoch [6/10], Training Loss: 0.0135\n",
      "Epoch [6/10], Validation Loss: 0.0381\n",
      "Epoch [6/10], Validation Accuracy: 98.9827%\n",
      "Epoch [7/10], Training Loss: 0.0589\n",
      "Epoch [7/10], Validation Loss: 0.0364\n",
      "Epoch [7/10], Validation Accuracy: 99.1751%\n",
      "Epoch [8/10], Training Loss: 0.0769\n",
      "Epoch [8/10], Validation Loss: 0.0395\n",
      "Epoch [8/10], Validation Accuracy: 99.0652%\n",
      "Epoch [9/10], Training Loss: 0.0244\n",
      "Epoch [9/10], Validation Loss: 0.0339\n",
      "Epoch [9/10], Validation Accuracy: 99.1202%\n",
      "Epoch [10/10], Training Loss: 0.0168\n",
      "Epoch [10/10], Validation Loss: 0.0385\n",
      "Epoch [10/10], Validation Accuracy: 99.1476%\n",
      "Epoch [1/10], Training Loss: 0.1036\n",
      "Epoch [1/10], Validation Loss: 0.0564\n",
      "Epoch [1/10], Validation Accuracy: 98.4603%\n",
      "Epoch [2/10], Training Loss: 0.0200\n",
      "Epoch [2/10], Validation Loss: 0.0431\n",
      "Epoch [2/10], Validation Accuracy: 98.7077%\n",
      "Epoch [3/10], Training Loss: 0.0177\n",
      "Epoch [3/10], Validation Loss: 0.0437\n",
      "Epoch [3/10], Validation Accuracy: 98.9002%\n",
      "Epoch [4/10], Training Loss: 0.0310\n",
      "Epoch [4/10], Validation Loss: 0.0339\n",
      "Epoch [4/10], Validation Accuracy: 99.1751%\n",
      "Epoch [5/10], Training Loss: 0.0129\n",
      "Epoch [5/10], Validation Loss: 0.0366\n",
      "Epoch [5/10], Validation Accuracy: 99.0102%\n",
      "Epoch [6/10], Training Loss: 0.0319\n",
      "Epoch [6/10], Validation Loss: 0.0460\n",
      "Epoch [6/10], Validation Accuracy: 98.7627%\n",
      "Epoch [7/10], Training Loss: 0.0551\n",
      "Epoch [7/10], Validation Loss: 0.0488\n",
      "Epoch [7/10], Validation Accuracy: 98.4878%\n",
      "Epoch [8/10], Training Loss: 0.0814\n",
      "Epoch [8/10], Validation Loss: 0.0519\n",
      "Epoch [8/10], Validation Accuracy: 98.7077%\n",
      "Epoch [9/10], Training Loss: 0.0127\n",
      "Epoch [9/10], Validation Loss: 0.0343\n",
      "Epoch [9/10], Validation Accuracy: 99.1202%\n",
      "Epoch [10/10], Training Loss: 0.0165\n",
      "Epoch [10/10], Validation Loss: 0.0349\n",
      "Epoch [10/10], Validation Accuracy: 99.2026%\n",
      "[0.9913803041880255, 0.992075362645065]\n",
      "0.00034752922851977175\n",
      "0.9917278334165452\n"
     ]
    }
   ],
   "source": [
    "#without dropout\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffa9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without l2-rugularization\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 0, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without weight_decay\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff091f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without batches\n",
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "batch_size = train.shape[0]\n",
    "input_dim = train.shape[1]\n",
    "output_dim = 4\n",
    "\n",
    "F1 =[]\n",
    "for i in range(30):\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    test_outputs = model_neural(test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    F1.append(f1_score(test_label, test_pred, average='weighted'))\n",
    "print(F1)\n",
    "print(np.std(F1))\n",
    "print(np.mean(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21688a",
   "metadata": {},
   "source": [
    "### Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54521ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_nn = {'layer_dim': 256, 'number_hidden_layer': 4, 'dropout_prob': 0.8, 'l2_regu': 1e-05, 'weight_decay': 0.0001, 'learning_rate': 0.001, 'batch_size': 128, 'num_epochs': 10}\n",
    "layer_dim = best_params_nn['layer_dim']\n",
    "number_hidden_layer = best_params_nn['number_hidden_layer']\n",
    "dropout_prob = best_params_nn['dropout_prob']\n",
    "l2_regu = best_params_nn['l2_regu']\n",
    "weight_decay = best_params_nn['weight_decay']\n",
    "learning_rate = best_params_nn['learning_rate']\n",
    "batch_size = best_params_nn['batch_size']\n",
    "num_epochs = best_params_nn['num_epochs']\n",
    "\n",
    "best_para = ({'n_estimators': 100, 'max_depth': 400, 'max_features': 400, 'bootstrap': False, 'class_weight': 'balanced', 'min_samples_leaf': 5}, 0, 0)\n",
    "\n",
    "dfs = []\n",
    "for i in range(30):\n",
    "    \n",
    "    train, train_label, test, test_label, val, val_label=split_dataset(level3, 0.9, 0.1, 0)\n",
    "    \n",
    "    model_rdf = train_random_forest(train, train_label,test, test_label,best_para[0])\n",
    "    model_neural = ModelClassification(input_dim, output_dim, layer_dim, number_hidden_layer, dropout_prob, l2_regu)\n",
    "    optimizer = torch.optim.Adam(model_neural.parameters(), lr = learning_rate, weight_decay=weight_decay)#lr : learning rate\n",
    "    train_model(model_neural, num_epochs, train, train_label, test, test_label, optimizer, batch_size)\n",
    "    \n",
    "    dfs.append(feature_importances(model_rdf, model_neural, mutual_data, level3, smoothness = 40, plot = 0))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "arrays = [df.to_numpy() for df in dfs]\n",
    "\n",
    "stacked_array = np.stack(arrays, axis=0)\n",
    "mean_values = np.mean(stacked_array, axis=0)\n",
    "std_values = np.std(stacked_array, axis=0)\n",
    "\n",
    "\n",
    "np.savetxt('mean.txt', mean_values)\n",
    "np.savetxt('std.txt', std_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(mean_values.shape[0]), mean_values[:, 0],label='Neural Network', color=\"red\", lw=2) \n",
    "\n",
    "plt.fill_between(\n",
    "    np.arange(mean_values.shape[0]),\n",
    "    mean_values[:, 0] - std_values[:, 0],\n",
    "    mean_values[:, 0] + std_values[:, 0],\n",
    "    color=\"red\",\n",
    "    alpha=0.3 \n",
    ")\n",
    "plt.plot(np.arange(mean_values.shape[0]), mean_values[:, 1], label='Mutual Information', color=\"green\", lw=2) \n",
    "plt.fill_between(\n",
    "    np.arange(mean_values.shape[0]),\n",
    "    mean_values[:, 1] - std_values[:, 1],\n",
    "    mean_values[:, 1] + std_values[:, 1],\n",
    "    color=\"green\",\n",
    "    alpha=0.3 \n",
    ")\n",
    "plt.plot(np.arange(mean_values.shape[0]), mean_values[:, 2], label='Random forest', color=\"blue\", lw=2) \n",
    "plt.fill_between(\n",
    "    np.arange(mean_values.shape[0]),\n",
    "    mean_values[:, 2] - std_values[:, 2],\n",
    "    mean_values[:, 2] + std_values[:, 2],\n",
    "    color=\"blue\",\n",
    "    alpha=0.3 \n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel('Positions')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Position importances')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
